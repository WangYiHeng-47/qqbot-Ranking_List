# -*- coding: utf-8 -*-
"""
NLP åˆ†ææ¨¡å—
åŒ…å«æƒ…æ„Ÿåˆ†æã€TF-IDF å…³é”®è¯æå–ã€äº’åŠ¨å…³ç³»åˆ†æ
"""

import json
import logging
import re
from typing import List, Dict, Tuple, Optional, Any
from collections import Counter, defaultdict
from dataclasses import dataclass

logger = logging.getLogger("NLPAnalyzer")

# å°è¯•å¯¼å…¥ä¾èµ–
try:
    import jieba
    import jieba.analyse
    JIEBA_AVAILABLE = True
except ImportError:
    JIEBA_AVAILABLE = False
    logger.warning("jieba æœªå®‰è£…")

try:
    from snownlp import SnowNLP
    SNOWNLP_AVAILABLE = True
except ImportError:
    SNOWNLP_AVAILABLE = False
    logger.warning("snownlp æœªå®‰è£…ï¼Œæƒ…æ„Ÿåˆ†æåŠŸèƒ½ä¸å¯ç”¨")

try:
    import networkx as nx
    NETWORKX_AVAILABLE = True
except ImportError:
    NETWORKX_AVAILABLE = False
    logger.warning("networkx æœªå®‰è£…ï¼Œäº’åŠ¨å…³ç³»å›¾åŠŸèƒ½ä¸å¯ç”¨")


@dataclass
class SentimentResult:
    """æƒ…æ„Ÿåˆ†æç»“æœ"""
    positive_ratio: float      # ç§¯ææ¯”ä¾‹
    negative_ratio: float      # æ¶ˆææ¯”ä¾‹
    neutral_ratio: float       # ä¸­æ€§æ¯”ä¾‹
    average_score: float       # å¹³å‡æƒ…æ„Ÿåˆ†æ•° (0-1)
    mood: str                  # æ•´ä½“æ°›å›´æè¿°
    total_messages: int        # åˆ†æçš„æ¶ˆæ¯æ•°


@dataclass
class UserInteraction:
    """ç”¨æˆ·äº’åŠ¨æ•°æ®"""
    from_user: int
    to_user: int
    count: int


class NLPAnalyzer:
    """NLP åˆ†æå™¨"""
    
    def __init__(self, stop_words: set = None):
        self.stop_words = stop_words or set()
        
        # æƒ…æ„Ÿé˜ˆå€¼
        self.POSITIVE_THRESHOLD = 0.6
        self.NEGATIVE_THRESHOLD = 0.4
    
    def extract_text_from_messages(self, messages: List[Dict]) -> List[str]:
        """ä»æ¶ˆæ¯åˆ—è¡¨ä¸­æå–çº¯æ–‡æœ¬"""
        texts = []
        for msg in messages:
            try:
                raw_content = msg.get('raw_content', '[]')
                if isinstance(raw_content, str):
                    segments = json.loads(raw_content)
                else:
                    segments = raw_content
                
                for seg in segments:
                    if seg.get('type') == 'text':
                        text = seg.get('data', {}).get('text', '').strip()
                        if text and len(text) > 1:
                            texts.append(text)
            except (json.JSONDecodeError, KeyError, TypeError):
                continue
        return texts
    
    def analyze_sentiment(self, texts: List[str]) -> Optional[SentimentResult]:
        """
        æƒ…æ„Ÿåˆ†æ
        è¿”å›æ•´ä½“æƒ…æ„Ÿå€¾å‘
        """
        if not SNOWNLP_AVAILABLE:
            logger.warning("snownlp ä¸å¯ç”¨ï¼Œè·³è¿‡æƒ…æ„Ÿåˆ†æ")
            return None
        
        if not texts:
            return None
        
        scores = []
        for text in texts:
            try:
                if len(text) < 2:
                    continue
                s = SnowNLP(text)
                scores.append(s.sentiments)
            except Exception:
                continue
        
        if not scores:
            return None
        
        # ç»Ÿè®¡æƒ…æ„Ÿåˆ†å¸ƒ
        positive = sum(1 for s in scores if s >= self.POSITIVE_THRESHOLD)
        negative = sum(1 for s in scores if s <= self.NEGATIVE_THRESHOLD)
        neutral = len(scores) - positive - negative
        
        total = len(scores)
        avg_score = sum(scores) / total
        
        # åˆ¤æ–­æ•´ä½“æ°›å›´
        if avg_score >= 0.65:
            mood = "ğŸŒˆ ç¾¤èŠæ°›å›´éå¸¸ç§¯ææ´»è·ƒï¼"
        elif avg_score >= 0.55:
            mood = "ğŸ˜Š ç¾¤èŠæ°›å›´æ¯”è¾ƒè½»æ¾æ„‰å¿«"
        elif avg_score >= 0.45:
            mood = "ğŸ˜ ç¾¤èŠæ°›å›´æ¯”è¾ƒå¹³æ·¡ä¸­æ€§"
        elif avg_score >= 0.35:
            mood = "ğŸ˜” ç¾¤èŠæ°›å›´æœ‰äº›ä½è½"
        else:
            mood = "ğŸ˜° ç¾¤èŠæ°›å›´æ¯”è¾ƒç„¦è™‘æ¶ˆæ"
        
        return SentimentResult(
            positive_ratio=positive / total,
            negative_ratio=negative / total,
            neutral_ratio=neutral / total,
            average_score=avg_score,
            mood=mood,
            total_messages=total
        )
    
    def extract_keywords_tfidf(self, texts: List[str], top_n: int = 20) -> List[Tuple[str, float]]:
        """
        ä½¿ç”¨ TF-IDF æå–å…³é”®è¯
        è¿”å›: [(å…³é”®è¯, æƒé‡), ...]
        """
        if not JIEBA_AVAILABLE:
            return []
        
        if not texts:
            return []
        
        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬
        full_text = " ".join(texts)
        
        # ä½¿ç”¨ jieba çš„ TF-IDF ç®—æ³•
        keywords = jieba.analyse.extract_tags(
            full_text,
            topK=top_n,
            withWeight=True,
            allowPOS=('n', 'nr', 'ns', 'nt', 'nz', 'v', 'vn', 'a', 'an')  # åè¯ã€åŠ¨è¯ã€å½¢å®¹è¯
        )
        
        # è¿‡æ»¤åœç”¨è¯
        filtered = [
            (word, weight) for word, weight in keywords
            if word not in self.stop_words and len(word) > 1
        ]
        
        return filtered[:top_n]
    
    def extract_keywords_textrank(self, texts: List[str], top_n: int = 20) -> List[Tuple[str, float]]:
        """
        ä½¿ç”¨ TextRank æå–å…³é”®è¯
        """
        if not JIEBA_AVAILABLE:
            return []
        
        if not texts:
            return []
        
        full_text = " ".join(texts)
        
        keywords = jieba.analyse.textrank(
            full_text,
            topK=top_n,
            withWeight=True,
            allowPOS=('n', 'nr', 'ns', 'nt', 'nz', 'v', 'vn')
        )
        
        filtered = [
            (word, weight) for word, weight in keywords
            if word not in self.stop_words and len(word) > 1
        ]
        
        return filtered[:top_n]
    
    def analyze_interactions(self, messages: List[Dict]) -> Dict[str, Any]:
        """
        åˆ†æç”¨æˆ·äº’åŠ¨å…³ç³»
        é€šè¿‡ @ å’Œå›å¤æ¶ˆæ¯æ¥åˆ¤æ–­äº’åŠ¨
        
        messages: [{'user_id': 123, 'raw_content': '...', 'reply_to': 456}, ...]
        """
        if not messages:
            return {'edges': [], 'nodes': [], 'stats': {}}
        
        # ç»Ÿè®¡äº’åŠ¨æ¬¡æ•°
        interactions = defaultdict(int)  # (from_user, to_user) -> count
        user_msg_count = defaultdict(int)  # user_id -> æ¶ˆæ¯æ•°
        
        for msg in messages:
            user_id = msg.get('user_id')
            if not user_id:
                continue
            
            user_msg_count[user_id] += 1
            
            # è§£æ @ çš„ç”¨æˆ·
            try:
                raw_content = msg.get('raw_content', '[]')
                if isinstance(raw_content, str):
                    segments = json.loads(raw_content)
                else:
                    segments = raw_content
                
                for seg in segments:
                    if seg.get('type') == 'at':
                        target_qq = seg.get('data', {}).get('qq')
                        if target_qq and target_qq != 'all':
                            try:
                                target_id = int(target_qq)
                                if target_id != user_id:  # æ’é™¤è‡ªå·± @ è‡ªå·±
                                    interactions[(user_id, target_id)] += 1
                            except ValueError:
                                pass
                    
                    # è§£æå›å¤æ¶ˆæ¯
                    if seg.get('type') == 'reply':
                        # å›å¤æ¶ˆæ¯éœ€è¦ä»æ•°æ®åº“æŸ¥è¯¢è¢«å›å¤æ¶ˆæ¯çš„å‘é€è€…
                        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œåç»­å¯æ‰©å±•
                        pass
                        
            except (json.JSONDecodeError, KeyError, TypeError):
                continue
        
        # æ„å»ºè¾¹åˆ—è¡¨
        edges = [
            {'from': from_user, 'to': to_user, 'weight': count}
            for (from_user, to_user), count in interactions.items()
            if count > 0
        ]
        
        # èŠ‚ç‚¹åˆ—è¡¨
        all_users = set(user_msg_count.keys())
        for edge in edges:
            all_users.add(edge['from'])
            all_users.add(edge['to'])
        
        nodes = [
            {'id': user_id, 'msg_count': user_msg_count.get(user_id, 0)}
            for user_id in all_users
        ]
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total_interactions': sum(e['weight'] for e in edges),
            'unique_pairs': len(edges),
            'most_active_pair': max(edges, key=lambda x: x['weight']) if edges else None
        }
        
        return {
            'edges': edges,
            'nodes': nodes,
            'stats': stats
        }
    
    def detect_repeaters(self, messages, min_repeat: int = 3) -> List[Dict]:
        """
        æ£€æµ‹å¤è¯»æœº
        
        å‚æ•°:
            messages: å¯ä»¥æ˜¯ä»¥ä¸‹æ ¼å¼ä¹‹ä¸€:
                - List[Tuple[str, int]]: [(æ–‡æœ¬, user_id), ...]
                - List[Dict]: [{'raw_content': ..., 'user_id': ...}, ...]
            min_repeat: æœ€å°è¿ç»­é‡å¤æ¬¡æ•°
        
        è¿”å›è¿ç»­ç›¸åŒæ¶ˆæ¯çš„ç»Ÿè®¡
        """
        if not messages:
            return []
        
        # æå–çº¯æ–‡æœ¬å†…å®¹
        msg_texts = []
        
        for msg in messages:
            # å¤„ç†ç®€å•å…ƒç»„æ ¼å¼ (text, user_id)
            if isinstance(msg, (tuple, list)) and len(msg) >= 2:
                text, user_id = msg[0], msg[1]
                if text and text.strip():
                    msg_texts.append({
                        'user_id': user_id,
                        'text': text.strip()
                    })
                continue
            
            # å¤„ç† dict æ ¼å¼
            if isinstance(msg, dict):
                try:
                    raw_content = msg.get('raw_content', '[]')
                    if isinstance(raw_content, str):
                        segments = json.loads(raw_content)
                    else:
                        segments = raw_content
                    
                    text_parts = []
                    for seg in segments:
                        if seg.get('type') == 'text':
                            text_parts.append(seg.get('data', {}).get('text', ''))
                    
                    full_text = ''.join(text_parts).strip()
                    if full_text:
                        msg_texts.append({
                            'user_id': msg.get('user_id'),
                            'text': full_text,
                            'time': msg.get('created_at', 0)
                        })
                except Exception:
                    continue
        
        if not msg_texts:
            return []
        
        # æ£€æµ‹è¿ç»­å¤è¯»
        repeats = []
        current_text = None
        current_users = []
        
        for item in msg_texts:
            if item['text'] == current_text:
                current_users.append(item['user_id'])
            else:
                if len(current_users) >= min_repeat:
                    repeats.append({
                        'text': current_text[:50] + ('...' if len(current_text) > 50 else ''),
                        'count': len(current_users),
                        'users': list(set(current_users))
                    })
                current_text = item['text']
                current_users = [item['user_id']]
        
        # å¤„ç†æœ€åä¸€ç»„
        if len(current_users) >= min_repeat:
            repeats.append({
                'text': current_text[:50] + ('...' if len(current_text) > 50 else ''),
                'count': len(current_users),
                'users': list(set(current_users))
            })
        
        # æŒ‰å¤è¯»æ¬¡æ•°æ’åº
        repeats.sort(key=lambda x: x['count'], reverse=True)
        
        return repeats
    
    def get_user_word_cloud(self, texts_or_messages, user_id: int = None, top_n: int = 30) -> List[Tuple[str, int]]:
        """
        è·å–è¯äº‘æ•°æ®
        
        Args:
            texts_or_messages: å¯ä»¥æ˜¯æ–‡æœ¬åˆ—è¡¨ List[str] æˆ–æ¶ˆæ¯åˆ—è¡¨ List[Dict]
            user_id: å¦‚æœä¼ æ¶ˆæ¯åˆ—è¡¨ï¼Œç”¨äºè¿‡æ»¤ç”¨æˆ·ï¼ˆå¯é€‰ï¼‰
            top_n: è¿”å›å‰ N ä¸ªè¯
        """
        if not JIEBA_AVAILABLE:
            return []
        
        user_texts = []
        
        # åˆ¤æ–­è¾“å…¥ç±»å‹
        if texts_or_messages and isinstance(texts_or_messages[0], str):
            # ç›´æ¥æ˜¯æ–‡æœ¬åˆ—è¡¨
            user_texts = texts_or_messages
        else:
            # æ˜¯æ¶ˆæ¯åˆ—è¡¨ï¼Œéœ€è¦æå–æ–‡æœ¬
            for msg in texts_or_messages:
                if user_id and msg.get('user_id') != user_id:
                    continue
                try:
                    raw_content = msg.get('raw_content', '[]')
                    if isinstance(raw_content, str):
                        segments = json.loads(raw_content)
                    else:
                        segments = raw_content
                    
                    for seg in segments:
                        if seg.get('type') == 'text':
                            text = seg.get('data', {}).get('text', '').strip()
                            if text:
                                user_texts.append(text)
                except Exception:
                    continue
        
        if not user_texts:
            return []
        
        # åˆ†è¯ç»Ÿè®¡
        words = []
        for text in user_texts:
            words.extend(jieba.lcut(text))
        
        # è¿‡æ»¤
        filtered = [
            w for w in words
            if len(w) > 1 and w not in self.stop_words
        ]
        
        # ç»Ÿè®¡
        counter = Counter(filtered)
        return counter.most_common(top_n)
    
    def analyze_user_active_hours(self, messages: List[Dict], user_id: int) -> Dict[int, int]:
        """
        åˆ†æç”¨æˆ·æ´»è·ƒæ—¶æ®µ
        è¿”å›: {hour: count, ...}
        """
        import time
        
        hourly = {i: 0 for i in range(24)}
        
        for msg in messages:
            if msg.get('user_id') != user_id:
                continue
            
            created_at = msg.get('created_at', 0)
            if created_at:
                hour = time.localtime(created_at).tm_hour
                hourly[hour] += 1
        
        return hourly
    
    def get_user_type(self, hourly_stats: Dict[int, int]) -> str:
        """
        æ ¹æ®æ´»è·ƒæ—¶æ®µåˆ¤æ–­ç”¨æˆ·ç±»å‹
        """
        if not hourly_stats or sum(hourly_stats.values()) == 0:
            return "ğŸ«¥ æ½œæ°´å‘˜"
        
        # è®¡ç®—å„æ—¶æ®µæ¶ˆæ¯å æ¯”
        total = sum(hourly_stats.values())
        
        morning = sum(hourly_stats.get(h, 0) for h in range(6, 12)) / total  # 6-12ç‚¹
        afternoon = sum(hourly_stats.get(h, 0) for h in range(12, 18)) / total  # 12-18ç‚¹
        evening = sum(hourly_stats.get(h, 0) for h in range(18, 24)) / total  # 18-24ç‚¹
        night = sum(hourly_stats.get(h, 0) for h in list(range(0, 6))) / total  # 0-6ç‚¹
        
        # åˆ¤æ–­ç±»å‹
        if night > 0.3:
            return "ğŸ¦‰ å¤œçŒ«å­"
        elif morning > 0.4:
            return "ğŸ¦ æ—©èµ·é¸Ÿ"
        elif evening > 0.5:
            return "ğŸŒ™ å¤œé—´æ´»è·ƒ"
        elif afternoon > 0.4:
            return "â˜€ï¸ åˆåè¾¾äºº"
        else:
            return "âš–ï¸ å‡è¡¡å‹"
